{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator, load_img\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport cv2\nimport glob\n\ndata = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')\ntotal_data = pd.concat([data])\npath = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'+total_data.image_id.loc[0]\n\n\ncopy_data = total_data[total_data.Male != ''].copy()\ncopy_data['Male'] = copy_data['Male'].replace([-1], 0)\ncopy = copy_data[:10000]\ncopy.info()\n#labels = copy_data.Male\n#print(labels.info())\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-03T23:48:52.805359Z","iopub.execute_input":"2022-05-03T23:48:52.805725Z","iopub.status.idle":"2022-05-03T23:48:55.805497Z","shell.execute_reply.started":"2022-05-03T23:48:52.805618Z","shell.execute_reply":"2022-05-03T23:48:55.804574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nlabels = []\nfrom keras.preprocessing.image import img_to_array\nimport numpy as np\nY = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'+copy.image_id\nfor img in Y:\n    image = cv2.imread(img)\n    image = cv2.resize(image,(96, 96))\n    image = img_to_array(image)\n    data.append(image)\nfor label1 in copy.Male:\n    if label1 == 0:\n        label = 0\n    else:\n        label = 1\n        \n    labels.append([label])\n            \n        \n    \n    \ndata = np.array(data, dtype='float') / 255.0\n#data = np.squeeze(data)/255\nlabels = np.array(labels)\nprint(data.shape)\nprint(labels.shape)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:49:04.967715Z","iopub.execute_input":"2022-05-03T23:49:04.968173Z","iopub.status.idle":"2022-05-03T23:49:22.824730Z","shell.execute_reply.started":"2022-05-03T23:49:04.968134Z","shell.execute_reply":"2022-05-03T23:49:22.823857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical, plot_model\n(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)\n\ntrainY = to_categorical(trainY, num_classes=2)\ntestY = to_categorical(testY, num_classes=2)\n\n#augmenting dataset\naug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:27.868273Z","iopub.execute_input":"2022-05-03T23:50:27.868981Z","iopub.status.idle":"2022-05-03T23:50:29.151653Z","shell.execute_reply.started":"2022-05-03T23:50:27.868942Z","shell.execute_reply":"2022-05-03T23:50:29.150333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\nfrom keras import backend as K\ndef build(width, height, depth, classes):\n    model = Sequential()\n    inputShape = (height, width, depth)\n    chanDim = -1\n\n    if K.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n        inputShape = (depth, height, width)\n        chanDim = 1\n    \n    # The axis that should be normalized, after a Conv2D layer with data_format=\"channels_first\", \n    # set axis=1 in BatchNormalization.\n\n    model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=inputShape))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=chanDim))\n    model.add(MaxPooling2D(pool_size=(3,3)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3,3), padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=chanDim))\n\n    model.add(Conv2D(64, (3,3), padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=chanDim))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (3,3), padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=chanDim))\n\n    model.add(Conv2D(128, (3,3), padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=chanDim))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    #model.add(GlobalAveragePooling2D())\n    model.add(Dense(1024))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(classes))\n    model.add(Activation(\"sigmoid\"))\n    model.summary()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:33.157575Z","iopub.execute_input":"2022-05-03T23:50:33.159332Z","iopub.status.idle":"2022-05-03T23:50:33.177802Z","shell.execute_reply.started":"2022-05-03T23:50:33.159252Z","shell.execute_reply":"2022-05-03T23:50:33.176818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build(width=96, height=96, depth=3,classes=2)\n#Complie model\nfrom tensorflow.keras.optimizers import Adam\nlr = 1e-3\nepochs = 35\nopt = Adam(learning_rate=lr, decay=lr/epochs)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:40.006961Z","iopub.execute_input":"2022-05-03T23:50:40.007802Z","iopub.status.idle":"2022-05-03T23:50:40.285725Z","shell.execute_reply.started":"2022-05-03T23:50:40.007767Z","shell.execute_reply":"2022-05-03T23:50:40.284972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train model\nbatch_size = 64\nH = model.fit(aug.flow(trainX, trainY, batch_size=batch_size),\n                        validation_data=(testX,testY),\n                        steps_per_epoch=len(trainX) // batch_size,\n                        epochs=epochs, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T23:50:49.389963Z","iopub.execute_input":"2022-05-03T23:50:49.390574Z","iopub.status.idle":"2022-05-04T01:07:34.972936Z","shell.execute_reply.started":"2022-05-03T23:50:49.390532Z","shell.execute_reply":"2022-05-04T01:07:34.970969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(testX, testY, verbose=0)\nprint('\\n', 'Test accuracy:', score[1]*100, '%')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T01:08:15.865301Z","iopub.execute_input":"2022-05-04T01:08:15.866590Z","iopub.status.idle":"2022-05-04T01:08:21.015698Z","shell.execute_reply.started":"2022-05-04T01:08:15.866519Z","shell.execute_reply":"2022-05-04T01:08:21.015062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels =[  \n        \"Female\",# index 0\n    \"Male\",# index 1\n        ]\ny_hat = model.predict(testX)\n\n# Plot a random sample of 10 test images, their predicted labels and ground truth\nfigure = plt.figure(figsize=(20, 8))\nfor i, index in enumerate(np.random.choice(testX.shape[0], size=15, replace=False)):\n    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n    # Display each image\n    ax.imshow(np.squeeze(testX[index]))\n    predict_index = np.argmax(y_hat[index])\n    true_index = np.argmax(testY[index])\n    # Set the title for each image\n    ax.set_title(\"{} ({})\".format(labels[predict_index], \n                                  labels[true_index]),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T01:09:20.375038Z","iopub.execute_input":"2022-05-04T01:09:20.375349Z","iopub.status.idle":"2022-05-04T01:09:26.960172Z","shell.execute_reply.started":"2022-05-04T01:09:20.375307Z","shell.execute_reply":"2022-05-04T01:09:26.959339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot training/validation loss/accuracy\nplt.style.use(\"ggplot\")\nplt.figure()\nN = epochs\nplt.plot(np.arange(0,N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0,N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0,N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0,N), H.history[\"val_accuracy\"], label=\"val_acc\")\n\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"upper right\")\n\n# save plot to disk\nplt.savefig('plot.png')","metadata":{"execution":{"iopub.status.busy":"2022-05-04T01:16:15.600704Z","iopub.execute_input":"2022-05-04T01:16:15.602816Z","iopub.status.idle":"2022-05-04T01:16:15.927569Z","shell.execute_reply.started":"2022-05-04T01:16:15.602688Z","shell.execute_reply":"2022-05-04T01:16:15.926832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nimport cv2\nimport os\nimport cvlib as cv\n                    \n# load model\nmodel = load_model('gender_detection.model')\n\n# open webcam\nwebcam = cv2.VideoCapture(0)\n    \nclasses = ['man','woman']\n\n# loop through frames\nwhile webcam.isOpened():\n\n    # read frame from webcam \n    status, frame = webcam.read()\n\n    # apply face detection\n    face, confidence = cv.detect_face(frame)\n\n\n    # loop through detected faces\n    for idx, f in enumerate(face):\n\n        # get corner points of face rectangle        \n        (startX, startY) = f[0], f[1]\n        (endX, endY) = f[2], f[3]\n\n        # draw rectangle over face\n        cv2.rectangle(frame, (startX,startY), (endX,endY), (0,255,0), 2)\n\n        # crop the detected face region\n        face_crop = np.copy(frame[startY:endY,startX:endX])\n\n        if (face_crop.shape[0]) < 10 or (face_crop.shape[1]) < 10:\n            continue\n\n        # preprocessing for gender detection model\n        face_crop = cv2.resize(face_crop, (96,96))\n        face_crop = face_crop.astype(\"float\") / 255.0\n        face_crop = img_to_array(face_crop)\n        face_crop = np.expand_dims(face_crop, axis=0)\n\n        # apply gender detection on face\n        conf = model.predict(face_crop)[0] # model.predict return a 2D matrix, ex: [[9.9993384e-01 7.4850512e-05]]\n\n        # get label with max accuracy\n        idx = np.argmax(conf)\n        label = classes[idx]\n\n        label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n\n        Y = startY - 10 if startY - 10 > 10 else startY + 10\n\n        # write label and confidence above face rectangle\n        cv2.putText(frame, label, (startX, Y),  cv2.FONT_HERSHEY_SIMPLEX,\n                    0.7, (0, 255, 0), 2)\n\n    # display output\n    cv2.imshow(\"gender detection\", frame)\n\n    # press \"Q\" to stop\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:23:11.894128Z","iopub.execute_input":"2022-05-03T14:23:11.894366Z","iopub.status.idle":"2022-05-03T14:23:11.932524Z","shell.execute_reply.started":"2022-05-03T14:23:11.894339Z","shell.execute_reply":"2022-05-03T14:23:11.931659Z"},"trusted":true},"execution_count":null,"outputs":[]}]}